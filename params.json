{
  "name": "Cs231nproject",
  "tagline": "Final project for my directed study CS231n.",
  "body": "In my past assignment I learned the concepts behind neural networks and then implemented my own. In this project I will again be implementing neural networks; however, instead of implementing my own code, I will be learning to work with neural network libraries. [Here](https://www.youtube.com/watch?v=Psp95I--hes&feature=youtu.be) is a screen cast I created going over my work in this project. \r\n##Deep Learning Libraries\r\nWhen actually working in deep learning, you generally do not write all of your own code, instead you utilize deep learning libraries. What a library does is best explained by looking at an equation for a neural net:\r\n\r\n![Linear classifier](https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/linear.png)\r\n\r\n\r\nWith f representing some function and x representing some input. What is changing here is f, and that is why you use a deep learning library for. Applying different functions to your data will yield different results and, if done correctly, better results. A deep learning library stores a whole bunch of functions that can be easily accessed. Further, libraries also make it really easy to mix and match these functions on top of each other and form multiple layer neural networks:\r\n\r\n![3 layer](https://raw.githubusercontent.com/georgewildridge/CS231n/master/Website%20Pictures/theelayer.png)\r\n \r\nWhat this allows you to do is \"design\" your neural networks, an idea I have not yet learned how to do but will definitely get to over the summer. For me this assignment was just about learning how to use a library. It is easier said than done, as figuring out how to install and then use the libraries can be tedious. My first goal was to run a neural network on the MNIST dataset using the deep learning library [torch](http://torch.ch/). In about 6 hours I was able to accomplish this using this [tutorial](http://dp.readthedocs.io/en/latest/neuralnetworktutoria l/index.html). In which I train a neural network on the MNIST dataset. The results after training were an error of  0.0008%. Better than a human!\r\n\r\nI also spent a few hours attempting to get another deep learning library called [Caffe](http://caffe.berkeleyvision.org/) up and running with the MNIST dataset. However, installation has proved tricky so I will finish it in the future. \r\n\r\n##Deep learning on Lord of the Rings!\r\nAfter this I found a really cool [project](https://github.com/karpathy/char-rnn) that Andrej Karpathy , the graduate student who taught CS231n, created using torch that trained a neural network on a large body of text and then attempted to write.  He trained a neural network on Shakespeare and got some awesome results so I figured I would give it a try using the Lord of the Rings trilogy. After 1000 iterations it outputted:\r\n\r\n\tthe itt no pounters here the casmose on the fillower.He mare and hail spide and quietiiss, and the smirnol. But not in sipar Back! the ever along what we ho angs headd heating and lave t he dromned.\r\n\tThearine in had a vadeereled word Sarcore of wave heTouch Ho bout! Lethlost clomend over in hizl whe for fagh to lake it for moby, And\r\n\t\r\n\r\n2000 iterations:\r\n\r\n\tfert; and they ithersed: whiits heme pacching sow. In the ring Picpl of the Lount heed plang wiet agetury turn-reveslous. I am jope over the younce by wasgancly the drey nepurst had ppinings. Then wery south a\r\n\tdrift. I the stommband\r\n\tlight then land, shat it swall, sowenthe\r\n\tfrodingtell, from there mark be mounto amight truge light things. But it trick\r\n\r\n3000 iterations: \r\n\r\n\tsh. He stooded to him. He could folk on the quiet him, and their comfant to the mole of the fither-leaves of Bag Forn led the ploce still made, howt\r\n\tof the Inlady that with had feir yards of the party and far ever or smill, and\r\n\tin the days were looked at out of hissoftle landering\r\n\tevil and other hobbits shust and other and far knowir that in the tumber of the Forest the\r\n\thobbits for the\r\n\t\r\n\t\r\n4000 iterations:\r\n\r\n\tIt shilld he did not get out of the mists, and the blue, side of Pippin when the griving over the stone if he turned between fair.\r\n\tNot destentes\r\n\tin the Wise moved it. The\r\n\tbidden was said, and filled on foot breath-forts.\r\n\tThe hobbits\r\n\twould make through the precetury,\r\n\tMo, though\r\n\tSprence\r\n\tshoulders could hust be time be shot that have been any south and had in \r\n\t\r\n\t\r\n5000 iterations:\r\n\r\n\tsh. It should not find him them all the path, like silence, and\r\n\tneighed in the honest in!20\r\n\tthe fellowship of\r\n\tthe ring\r\n\tlooking more to be no more. Nor change or markwours again, hu! Or if you think I have found to me. This valley reachwate mean through the road \r\n\t\r\nThe visible improvement the neural networks made was really cool! The difference between the 1000th iteration and the 5000th iteration is astounding. By 5000 iterations the network has learned to form English words well, and it is beginning to decipher the nuance of grammar within the English language. \r\n\r\n##Conclusion\r\nI am really happy with the results of this project as I believe it served as a great introduction into learning to work with a deep learning library. It has also illuminated some possible future work for me to do like learning how to design a neural network and looking into other deep learning libraries. \r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}